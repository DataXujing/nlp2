<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>word2vec - NLP-2</title>
  

  <link rel="shortcut icon" href="../icon.ico">
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">

  
  <script>
    // Current page data
    var mkdocs_page_name = "word2vec";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script>
  <script src="../js/theme.js"></script> 

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> NLP-2</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">主页</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../chapter1/">TF-IDF</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../chapter2/">Embadding</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href="./">word2vec</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#1">1.八卦时间</a></li>
                
            
                <li class="toctree-l3"><a href="#2word2vec">2.什么是word2vec</a></li>
                
            
                <li class="toctree-l3"><a href="#3">3.模型</a></li>
                
                    <li><a class="toctree-l4" href="#acbow">A.CBOW</a></li>
                
                    <li><a class="toctree-l4" href="#bskip-gram">B.Skip-Gram</a></li>
                
            
                <li class="toctree-l3"><a href="#4tricks">4.Tricks</a></li>
                
            
                <li class="toctree-l3"><a href="#5">5.小结</a></li>
                
            
            </ul>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../chapter4/">seq2seq</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../chapter5/">text2vec</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../about/">关于</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">NLP-2</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>word2vec</li>
    <li class="wy-breadcrumbs-aside">
      
        
          <a href="https://github.com/DataXujing/nlp2/" class="icon icon-github"> Edit on GitHub</a>
        
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="1">1.八卦时间</h2>
<div align=center>
<img src="../img/niu.png" />
</div>

<p>除了bengio之外, 不得不提的一个名字是：Jürgen Schmidhuber</p>
<hr />
<h2 id="2word2vec">2.什么是word2vec</h2>
<ul>
<li>
<p>word2vec是一个将单词转换成向量形式的工具。可以把对文本内容的处理简化为向量空间中的向量运算，计算出向量空间上的相似度，来表示文本语义上的相似度, </p>
</li>
<li>
<p>是由Google 2013年开源的项目。</p>
</li>
<li>
<p>采用了新的Log-Bilinear模型主要有两个：CBOW和Skip-Gram</p>
</li>
</ul>
<hr />
<h2 id="3">3.模型</h2>
<p>NNLM模型只能处理定长的序列。在03年的论文里，Bengio等人将模型能够一次处理的序列长度NN提高到了55，虽然相比bigram和trigram已经是很大的提升，但依然缺少灵活性。</p>
<p>因此，Mikolov等人在2010年提出了一种RNNLM模型，用递归神经网络代替原始模型里的前向反馈神经网络，并将embedding层与RNN里的隐藏层合并，从而解决了变长序列的问题。</p>
<p>另一个问题就比较严重了。NNLM的训练太慢了。即便是在百万量级的数据集上，即便是借助了40个CPU进行训练，NNLM也需要耗时数周才能给出一个稍微靠谱的解来。显然，对于现在动辄上千万甚至上亿的真实语料库，训练一个NNLM模型几乎是一个不可能的事情。</p>
<p>这时候，还是那个Mikolov站了出来。他注意到，原始的NNLM模型的训练其实可以拆分成两个步骤：</p>
<p>用一个简单模型训练出连续的词向量；
基于词向量的表达，训练一个连续的Ngram神经网络模型。
而NNLM模型的计算瓶颈主要是在第二步。
如果我们只是想得到word的连续特征向量，是不是可以对第二步里的神经网络模型进行简化呢？</p>
<p>Mikolov是这么想的，也是这么做的。他在2013年一口气推出了两篇paper，并开源了一款计算词向量的工具——至此，word2vec横空出世，主角闪亮登场。</p>
<div align=center>
<img src="../img/23.png" />
</div>

<p>有了前文的基础，理解word2vec算法就变得很简单了</p>
<h3 id="acbow">A.CBOW</h3>
<p>首先，我们对原始的NNLM模型做如下改造：</p>
<ul>
<li>
<p>移除前向反馈神经网络中非线性的hidden layer，直接将中间层的embedding layer与输出层的softmax layer连接；</p>
</li>
<li>
<p>忽略上下文环境的序列信息：输入的所有词向量均汇总到同一个embedding layer；</p>
</li>
<li>
<p>将future words纳入上下文环境</p>
</li>
</ul>
<p>得到的模型称之为CBoW模型（Continuous Bag-of-Words Model），也是word2vec算法的第一个模型：</p>
<div align=center>
<img src="../img/19.png" />
</div>

<div align=center>
<img src="../img/24.png" />
</div>

<p>从数学上看，CBOW模型等价于一个词袋模型的向量乘以一个embedding矩阵，从而得到一个连续的embedding向量。这也是CBOW模型名称的由来。</p>
<p>CBOW模型依然是从context对target word的预测中学习到词向量的表达。反过来，我们能否从target word对context的预测中学习到word vector呢？答案显然是可以的。</p>
<h3 id="bskip-gram">B.Skip-Gram</h3>
<div align=center>
<img src="../img/20.png" />
</div>

<div align=center>
<img src="../img/21.png" />
</div>

<div align=center>
<img src="../img/22.png" />
</div>

<div align=center>
<img src="../img/25.png" />
</div>

<hr />
<h2 id="4tricks">4.Tricks</h2>
<p>两种模型在训练的过程中采用了两种算法：层次softmax和Negative Sampling，
同时在Google开源的word2vec算法中使用了Huffman编码，指数运算，按word分布随机抽样，高频词亚采样等概念，关于这里的Trick我在6月份NLP的培训中已经做了详细的介绍，不赘述。</p>
<p>相关Tick感兴趣自己去查。</p>
<hr />
<h2 id="5">5.小结</h2>
<p>word2vec是一种快速把文本切词后的语料快速转化成词向量的方法，说白了是一种高效的词表示方法，主要模型就是我在上文中介绍的CBOW和Skip-Gram。
word2vec模型如何在R和Python中应用我在6月份的NLP培训中已经详细的讲过，不再赘述。</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../chapter4/" class="btn btn-neutral float-right" title="seq2seq"/>Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../chapter2/" class="btn btn-neutral" title="Embadding"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../chapter2/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../chapter4/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>

</body>
</html>
